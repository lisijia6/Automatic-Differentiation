{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OeHt8HxYGq3"
   },
   "source": [
    "# **Team 01 - Documentation**\n",
    "Team Members: Sijia (Nancy) Li, Qingyang (Catherine) Ni, Yuqing (Lily) Pan, Jiashu Xu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDC_G_11YD6u"
   },
   "source": [
    "## **1.0 Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7CGodBe7Ptt"
   },
   "source": [
    "Differentiation is the operation of finding derivatives of a given function with applications in many disciplines. In classical mechanics, time derivatives are particularly useful in describing physical processes. For example, the first derivative of an object's change in position (displacement) with respect to time is the velocity. The second derivative of an object's displacement is the acceleration. Another famous example is Euler equations, a system of conservation laws (i.e., conservation of mass, momentum, and energy) given by three partial differential equations. In optimization, differentiation can be used to find minima and maxima of functions. For a single variable, critical or stationary points are points in which the first derivative is zero. In higher dimensions, critical points are points in which the gradient is zero. Another useful technique is the second derivative test, whereby the second derivative of a single variable function or the eigenvalues of the Hessian matrix (a scalar-function's second partial derivatives) in higher dimensions can be used to test for local minimum and local maximum. In deep learning, optimization is used to train neural networks by finding a set of optimal weights that minimizes the loss function.\n",
    "\n",
    "In particular, there are three techniques for computing derivatives: symbolic differentiation, numerical differentiation, and automatic differentiation. Symbolic differentiation finds derivatives by applying various differentiation rules (e.g., sum rule, constant rule, power rule) to break complex functions into simpler expressions. However, simplification of a complex function or a large number of functions can lead to an exponentially large number of expressions to evaluate. This means that although symbolic differentiation will yield accurate results, it can be too slow and costly to perform such computations in real-life. Numerical differentiation estimates the derivatives from known values of the functions (e.g., finite difference approximation method). However, this technique can lead to inaccurate results due to floating point errors. Automatic differentiation (or algorithmic differentiation) solves these problems. Specifically, automatic differentiation breaks down the original functions into elementary operations (e.g., addition, subtraction, multiplication, division, exponentiation, and trigonometric functions) and then evaluate the chain rule step by step. This technique is able to compute derivatives efficiently (with lower cost than symbolic differentiation) at machine precision (more accurate than numerical differentiation). Since derivatives are ubiquitous in many real-world applications, it is important to have a useful tool to perform differentiation. Naturally, the best choice among the three aforementioned methods for computing derivatives is automatic differentiation, and `AutoDiff` is a Python package that implements this method.\n",
    "\n",
    "Three additional features are also implemented in the `AutoDiff` package: Reverse Mode, Optimization, and Computation Graph Visualization. The Reverse Mode for automatic differentiation is a two-pass process for recovering the partial derivatives. Optimization is a key concept in many fields (e.g., engineering, finance) and it helps us find the best possible solutions to a wide range of problems. In particular, Newton's Method and Stochastic Gradient Descent (SGD) are implemented. Finally, the `graphviz` software is used to develop the computation graph given an input function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XatWhgOoYXK7"
   },
   "source": [
    "## **2.0 Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kjh5JMrB7biI"
   },
   "source": [
    "### **2.1 The Chain Rule**\n",
    "The chain rule helps differentiate composition functions. Applying the chain rule, the derivative of a function $f(g(x))$ with respect to a single independent variable $x$ is given by:\n",
    "\n",
    "$\\frac{df}{dx} = \\frac{\\partial f}{\\partial g} \\frac{dg}{dx}$\n",
    "\n",
    "For example, given $f(g(x)) = cos(2x)$ and $g(x)=2x$, then $\\frac{\\partial f}{\\partial g}=-sin(g), \\frac{dg}{dx}=2$, and $\\frac{df}{dx}=-2sin(2x)$. For functions with two or more variables, we are interested in the gradient of a function $f(\\mathbf{x})$ with respect to all independent variables $\\mathbf{x}$, $\\nabla_{x}f$. Applying the chain rule, this is given by:\n",
    "\n",
    "$\\nabla_{\\mathbf{x}}f = \\sum_{i=1}^{n} \\frac{\\partial f}{\\partial g_i} \\nabla g_i(\\mathbf{x})$\n",
    "\n",
    "where $g_i$'s are functions with $m$ variables, $i={1,2,...,n}$. For example, given $f(g_1(\\mathbf{x}), g_2(\\mathbf{x})) = sin(g_1)-2g_2$, and $g_1(\\mathbf{x})=x_{1}+2x_{2}, g_2(\\mathbf{x})=3x_{1}^2x_2$, then $\\nabla g_1(\\mathbf{x})=\\begin{bmatrix} 1\\\\ 2 \\end{bmatrix}, \\nabla g_2(\\mathbf{x})=\\begin{bmatrix} 6x_1x_2\\\\ 3x_1^2 \\end{bmatrix}$, and $\\nabla_{\\mathbf{x}}f = \\frac{\\partial f}{\\partial g_1} \\nabla g_1(\\mathbf{x}) + \\frac{\\partial f}{\\partial g_2} \\nabla g_2(\\mathbf{x}) = cos(x_{1}+2x_{2})\\begin{bmatrix} 1\\\\ 2 \\end{bmatrix} - 2(3x_{1}^2x_2)\\begin{bmatrix} 6x_1x_2\\\\ 3x_1^2 \\end{bmatrix}$. This is an example with $m=2$ and $n=2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdx8Uq7kJ3P9"
   },
   "source": [
    "### **2.2 Elementary Functions**\n",
    "An elementary function is a combination of elementary operations, and constant, algebraic, exponential, and logarithmic functions as well as their inverses, e.g., $2x, e^x, sin(x), x+5$. We can decompose a function into smaller parts (elementary functions) in which their symbolic derivatives can be easily computed. The table below shows some examples of elementary functions with their respective derivatives. \n",
    "\n",
    "\n",
    "|**Elementary Function**| **Derivative of the Function**|\n",
    "|---------|---------\n",
    "|$c$| $0$|\n",
    "|$ax$| $a$|\n",
    "|$x^2$| $2x$|\n",
    "|$e^x$| $e^x$|\n",
    "|$ln(x)$| $\\frac{1}{x}$|\n",
    "|$sin(x)$| $cos(x)$|\n",
    "|$cos(x)$| $-sin(x)$|\n",
    "|$tan(x)$| $sec^2(x)$|\n",
    "|$cf$| $cf'$|\n",
    "|$x^n$| $nx^{n-1}$|\n",
    "|$f+g$| $f'+g'$|\n",
    "|$f-g$| $f'-g'$|\n",
    "|$fg$| $fg'+f'g$|\n",
    "|$f/g$| $\\frac{f'g-g'f}{g^2}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQUFz_7HJ6kO"
   },
   "source": [
    "### **2.3 Forward Mode**\n",
    "\n",
    "#### **2.3.1 Forward Primal Trace**\n",
    "Take the function $f(x_1,x_2)=sin(3x_1)+2(x_2)^3$ as an example. We develop the forward primal trace by finding intermediate results $v_j$ in which $j$ represents an elementary operation. This is achieved by working from the inside out. Given an arbitrary point $(x_1,x_2)$, we can evaluate the intermediate results at the point. The following table show the forward primal trace of the function $f(x_1,x_2)=sin(3x_1)+2(x_2)^3$ evaluated at the point $(\\frac{\\pi}{6},2)$.\n",
    "\n",
    "\n",
    "Intermediate  | Elementary Operation | Numerical Value\n",
    "------------- | ------------- | ------------- \n",
    "$v_{-1}=x_1$ | $\\frac{\\pi}{6}$| $0.52359877559$\n",
    "$v_0=x_2$ | $2$ |  $2$\n",
    "$v_1$ | $3v_{-1}$ | $1.57079632679$\n",
    "$v_2$ | $v_0^3$ | $8$\n",
    "$v_3$ | $sin(v_1)$ | $1$\n",
    "$v_4$ | $2v_2$ | $16$\n",
    "$v_5$ | $v_3+v_4$ | $17$\n",
    "\n",
    "\n",
    "#### **2.3.2 Computational (Forward) Graph**\n",
    "The computational graph is a way of visualizing the partial ordering of elementary operations with each node representing an intermediate result. The computational graph for the aforementioned function $f(x_1,x_2)=sin(3x_1)+2(x_2)^3$ is shown below.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1FgrTm0GWQrVszIgGo1wMs01Co-EJfqe1)\n",
    "\n",
    "#### **2.3.3 Forward Tangent Trace**\n",
    "Along with the forward primal trace, we also develop the forward tangent trace simultaneously by computing the directional derivative for each intermediate variable $D_pv_j$. By definition, the directional derivative is given by the following equation.\n",
    "\n",
    "$$D_pv_j = (\\nabla v_j)^Tp = \\sum_{i=1}^{m} \\frac{\\partial v_j}{\\partial x_i} p_i$$\n",
    "\n",
    "In this definition, $p$ is a $m$-dimensional seed vector in which $m$ is the number of independent variables. We can specify the derivative of interest using this seed vector. For example, if the goal is to find $\\frac{\\partial v_j}{\\partial x_5}$, then the element $p_5$ will be one and the remaining elements in the $p$ vector will be zero. The seed vector can be freely selected by the user. Generalizing this, the forward mode automatic differentiation is in essence computing $\\nabla f \\cdot p$ for a scalar function $f(x)$ and $J \\cdot p$ for a vector function $f(x)$ ($J$ is the Jacobian).\n",
    "\n",
    "The following table shows the forward primal trace and the forward tangent trace evaluated for the seed vectors $p_1=\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ and $p_2=\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\n",
    "\n",
    "Forward Primal Trace  | Forward Tangent Trace | $\\textbf{Pass } p^{(j=1)} = [1, 0]^T$ | $\\textbf{Pass } p^{(j=2)} = [0, 1]^T$\n",
    "------------- | ------------- | ------------- | ------------- \n",
    "$v_{-1}=x_1=\\frac{\\pi}{6}$ | $D_pv_{-1}=p_1$ | $D_pv_{-1}=1$ | $D_pv_{-1}=0$\n",
    "$v_0=x_2=2$ | $D_pv_{0}=p_2$ |  $D_pv_{0}=0$ | $D_pv_{0}=1$\n",
    "$v_1=3v_{-1}=\\frac{\\pi}{2}$ | $D_pv_{1}=3D_pv_{-1}$ | $D_pv_{1}=3$ | $D_pv_{1}=0$\n",
    "$v_2=v_0^3=8$ | $D_pv_{2}=3v_0^2D_pv_{0}$ | $D_pv_{2}=0$ | $D_pv_{2}=12$\n",
    "$v_3=sin(v_1)=1$ | $D_pv_{3}=cos(v_1)D_pv_{1}$ | $D_pv_{3}=0$ | $D_pv_{3}=0$\n",
    "$v_4=2v_2=16$ | $D_pv_{4}=2D_pv_{2}$ | $D_pv_{4}=0$ | $D_pv_{4}=24$\n",
    "$v_5=v_3+v_4=17$ | $D_pv_{5}=D_pv_{3}+D_pv_{4}$ | $D_pv_{5}=0$ | $D_pv_{5}=24$\n",
    "\n",
    "#### **2.3.4 Dual Numbers**\n",
    "By definition, a dual number is given by the equation $z = a + b\\epsilon$, where $a, b \\in \\mathbb{R}$ and $\\epsilon$ is a very small number not equal to zero such that $\\epsilon^2=0$. $a$ is the real part and $b$ is the dual part. This structure is very helpful in encoding the primal trace and tangent trace in forward mode automatic differentiation. The primal trace can be encoded by the real part and the tangent trace can be encoded by the dual part, hence the equation $z_j = v_j + D_pv_j\\epsilon$.\n",
    "\n",
    "Going back to the example, we can compute the last intermediate state using dual numbers. The last intermediate state is $z_5$ which is equal to $z_3+z_4$, where z_3, z_4, and z_5 are dual numbers. Let $z_3=a_3+b_3\\epsilon$ and $z_4=a_4+b_4\\epsilon$, then $z_5=(a_3+b_3\\epsilon) + (a_4+b_4\\epsilon) = (a_3+a_4) + (b_3+b_4)\\epsilon$, where $a_4+b_4\\epsilon$ is the real part and $b_3+b_4$ is the dual part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZBxllEOVli_"
   },
   "source": [
    "### **2.4 Reverse Mode**\n",
    "\n",
    "The reverse mode in automatic differentiation is a two-pass process for recovering the partial derivatives $\\frac{\\partial f_i}{\\partial v_{j-m}}$, where $f_i$'s are output functions and $v_{j-m}$ are the intermediate variables. These partial derivatives are called the adjoints. In other words, $\\bar v_{j-m}=\\frac{\\partial f_i}{\\partial v_{j-m}}$ and $\\bar v_{j-m}$ is the adjoint of $v_{j-m}$. The forward pass in the reverse mode will compute the the change in child node $v_j$ with respect to $v_j$'s parent node(s) $v_i$, denoted as $\\frac{\\partial v_j}{\\partial v_{i}}$. After that, the reverse pass will build up the chain rule using the formula $\\bar v_i = \\bar v_i + \\frac{\\partial v_f}{\\partial v_{j}} \\frac{\\partial v_j}{\\partial v_{i}} = \\bar v_i + \\bar v_j\\frac{\\partial v_j}{\\partial v_{i}}$, with $\\bar v_i$ initialized to zero for all $i$ and node $j$ is a child of node $i$. The last intermediate state is always equal to $1$ as the last nodes has no children. The following table illustrates the reverse mode computation for the same example in Section 2.3: $f(x_1,x_2)=sin(3x_1)+2(x_2)^3$ evaluated at the point $(\\frac{\\pi}{6},2)$.\n",
    "\n",
    "Forward Pass  |  | Reverse Pass \n",
    "------------- | ------------- | -------------\n",
    "**Intermediate** | **Partial Derivative** | **Adjoint**\n",
    "$v_{-1}=x_1=\\frac{\\pi}{6}$ | | $\\bar v_{-1} = \\bar v_{-1} + \\bar v_1 \\frac{\\partial v_1}{\\partial v_{-1}} = 0$\n",
    "$v_0=x_2=2$ | | $\\bar v_0 = \\bar v_0 + \\bar v_2 \\frac{\\partial v_2}{\\partial v_0} = 24$\n",
    "$v_1=3v_{-1}=\\frac{\\pi}{2}$ | $\\frac{\\partial v_1}{\\partial v_{-1}}=3$ | $\\bar v_1 = \\bar v_1 + \\bar v_3 \\frac{\\partial v_3}{\\partial v_1} = 0$\n",
    "$v_2=v_0^3=8$ | $\\frac{\\partial v_2}{\\partial v_{0}}=3v_0^2=12$ | $\\bar v_2 = \\bar v_2 + \\bar v_4 \\frac{\\partial v_4}{\\partial v_2} = 2$\n",
    "$v_3=sin(v_1)=1$ | $\\frac{\\partial v_3}{\\partial v_{1}}=cos(v_1)=0$ | $\\bar v_3 = \\bar v_3 + \\bar v_5 \\frac{\\partial v_5}{\\partial v_4} = 1$\n",
    "$v_4=2v_2=16$ | $\\frac{\\partial v_4}{\\partial v_{2}}=2$ | $\\bar v_4 = \\bar v_4 + \\frac{\\partial f}{\\partial v_5} \\frac{\\partial v_5}{\\partial v_4}= \\bar v_4 + \\bar v_5 \\frac{\\partial v_5}{\\partial v_4} = 0+1 = 1$\n",
    "$v_5=v_3+v_4=17$ | $\\frac{\\partial v_5}{\\partial v_{3}}=1; \\frac{\\partial v_5}{\\partial v_{4}}=1$ | $\\bar v_5 = \\frac{\\partial f}{\\partial v_5} \\frac{\\partial v_5}{\\partial v_5}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.5 Optimization**\n",
    "\n",
    "#### **2.5.1 Newton's Method**\n",
    "Newton's Method is an algorithm that computes an approximate solution $x^*$ to the equation $f(x) = 0$, given that the function $f$ is differentiable. The algorithm runs as follows.\n",
    "\n",
    "**Algorithm** (Newton's Method): <br>\n",
    "Input: initial guess $x_0$, input function $f$, maximum iteration $max\\_iter$, tolerance $tol$ <br>\n",
    "Output: minimum or maximum of function $f$\n",
    "\n",
    "&emsp; while $k < max\\_iter$ do <br>\n",
    "&emsp;&emsp; if $f(x_k) < tol$ then <br>\n",
    "&emsp;&emsp;&emsp; $x^*$ = $x_k$ <br>\n",
    "&emsp;&emsp;&emsp; return $x^*$ <br>\n",
    "&emsp;&emsp; end <br> <br>\n",
    "&emsp;&emsp; if scalar function then <br>\n",
    "&emsp;&emsp;&emsp; $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$ <br>\n",
    "&emsp;&emsp; else <br>\n",
    "&emsp;&emsp;&emsp; $J_f(x_k) \\Delta x_k = -F(x_k)$ <br>\n",
    "&emsp;&emsp;&emsp; $x_{k+1} \\gets x_k + \\Delta x_k$ <br>\n",
    "&emsp;&emsp; end <br>\n",
    "&emsp;&emsp; $k = k + 1$ <br>\n",
    "&emsp; end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.5.2 Stochastic Gradient Descent (SGD)**\n",
    "Stochastic Gradient Descent (SGD) is an algorithm for optimizing an objective function by iteratively descending in the negative direction of the gradient. The algorithm runs as follows.\n",
    "\n",
    "**Algorithm** (SGD): <br>\n",
    "Input: initial guess $x_0$, input function $f$, learning rate $\\eta$, maximum iteration $max\\_iter$, tolerance $tol$ <br>\n",
    "Output: minimum or maximum of function $f$\n",
    "\n",
    "&emsp; while $k < max\\_iter$ do <br>\n",
    "&emsp;&emsp; if $f(x_k) < tol$ then <br>\n",
    "&emsp;&emsp;&emsp; $x^*$ = $x_k$ <br>\n",
    "&emsp;&emsp;&emsp; return $x^*$ <br>\n",
    "&emsp;&emsp; end <br>\n",
    "&emsp;&emsp; $x_{k+1} \\gets x_k - \\eta \\Delta f(x_k)$ <br>\n",
    "&emsp;&emsp; $k = k + 1$ <br>\n",
    "&emsp; end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.6 The `graphviz` Software**\n",
    "`graphviz` is an open-source visualization software for representing graphs and networks. The computation graph of functions can be seen as a graph in which all the variables are nodes on the graph and the connections between variables are edges on the graph. The elementary operations can be seen as edge names in the graph. The documentation for the `graphviz` software can be found [here](https://graphviz.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuPtJdJ-YXGx"
   },
   "source": [
    "## **3.0 How to Use the `AutoDiff` Package**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 Installation**\n",
    "#### **3.1.1 User Installation Guide**\n",
    "##### **3.1.1.1 Manual Installation**\n",
    "We publish `AutoDiff` on the testPyPI, and user can simply install `AutoDiff` and its dependency by the following command. Note that the name `AutoDiff` has already been taken on PyPI, therefore, the name `AutoDiff-Team01` is used instead.\n",
    "\n",
    "```shell\n",
    "pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple AutoDiff-Team01\n",
    "```\n",
    "\n",
    "We have additional feature to visualize computation graph. To use this functionality user can install graphviz [here](https://graphviz.org/download/) and install python wrapper by the following command \n",
    "```\n",
    "pip install graphviz\n",
    "```\n",
    "##### **3.1.1.2 Installation Using Custom Docker**\n",
    "Alternatively, users can skip the above installation (for example if graphviz is not available in the machine). Users can download the Docker file named `Dockerfile` in our repo. Inside the directoy that contains the `Dockerfile`, users can build docker container, demonstrated by the following commands\n",
    "\n",
    "```shell\n",
    "# go to the directory that contains our Dockerfile\n",
    "cd <directory with Dockerfile>\n",
    "docker build .\n",
    "# for example, if the above command ends with\n",
    "#    Successfully built d8b31c5835d6  \n",
    "# the container id should be d8b31c5835d6\n",
    "docker run -it <container id>\n",
    "# then you can use our package in docker!\n",
    "```\n",
    "\n",
    "##### **3.1.1.3 Installation Using Our Docker**\n",
    "We also build the container and provide the built container that contains all depdendencies and AutoDiff on docker hub: [13052423200/autodiff](https://hub.docker.com/repository/docker/13052423200/autodiff). Please run \n",
    "```shell\n",
    "docker pull 13052423200/autodiff\n",
    "# then you can use our package in docker!\n",
    "```\n",
    "\n",
    "### **3.1.2 Developer Installation Guide**\n",
    "\n",
    "Developers can git clone our repo and install from the source using the following command\n",
    "```shell\n",
    "git clone git@code.harvard.edu:CS107/team01.git\n",
    "```\n",
    "\n",
    "Then excute the following command\n",
    "```shell\n",
    "cd team01\n",
    "# for main functionalty\n",
    "pip install .\n",
    "```\n",
    "If developers would like to  explore additional features such as computational graph plotting, as well as testing:\n",
    "\n",
    "```shell\n",
    "# for all features including test and plot\n",
    "pip install .[all]\n",
    "```\n",
    "\n",
    "Developers can also build or pull docker, please see user Installation guide above.\n",
    "\n",
    "### **3.2 Usage**\n",
    "Please see examples of how to use our package in the `example` folder. Forward Mode AD and Reverse Mode AD examples are given in `forward_mode.py` and `reverse_mode.py` respectively. Computation graph example is given in `plot_graph.py`. And driver code for optimization methods is provided in `newton.py` and `sgd.py` corresponding to Newton's Method and Stochastic Gradient Descent respectively.\n",
    "\n",
    "For more examples, please refer to `docs/documentation.pdf` section **3.0 How to Use the `AutoDiff` Package**, or view the full documentation with examples [here](https://code.harvard.edu/pages/CS107/team01/).\n",
    "\n",
    "For developer, checkout `test` folder for more examples and usages.\n",
    "\n",
    "The following sample code snippet demonstrates how users can interact with our package.\n",
    "    \n",
    "#### **3.2.1 Example 1: univariate scalar function**\n",
    "```python\n",
    ">>> from AutoDiff import Forward, Reverse\n",
    ">>> import numpy as np\n",
    ">>> # Create a node x with value 5.\n",
    ">>> x = 5\n",
    ">>> # Create a function y = exp(cos(x)+2).\n",
    ">>> f = lambda x: np.exp(np.cos(x)+2)\n",
    ">>> # Create a forward mode instance.\n",
    ">>> g = Forward(f, x)\n",
    ">>> # Evaluate the value of y at x=5\n",
    ">>> g.val\n",
    "9.812550066983217\n",
    ">>> # Calculate dy/dx = −exp(2+cos(x))sin(x) for x = 5.\n",
    ">>> g.der\n",
    "array([9.40949246])\n",
    ">>> # Create a reverse mode instance.\n",
    ">>> rev = Reverse(f, x)\n",
    ">>> # Evaluate the value of y at x=5\n",
    ">>> rev.val\n",
    "9.812550066983217\n",
    ">>> # Calculate dy/dx = −exp(2+cos(x))sin(x) for x = 5.\n",
    ">>> rev.der\n",
    "array([9.40949246])\n",
    "```\n",
    "\n",
    "#### **3.2.2 Example 2: multivariate scalar function**\n",
    "```python\n",
    ">>> from AutoDiff import Forward, Reverse\n",
    ">>> import numpy as np\n",
    ">>> # Create two nodes x1 with value 5 and x2 with value 3.\n",
    ">>> x = [5, 3]\n",
    ">>> # Create a function y = exp(cos(x1)+2sin(x2)).\n",
    ">>> def f(x1, x2):\n",
    "        return np.exp(np.cos(x1)+2*np.cos(x2))\n",
    ">>> # Create a forward mode instance.\n",
    ">>> g = Forward(f, *x)\n",
    ">>> # Evaluate the value of y at x1 = 5, x2 = 3\n",
    ">>> g.val\n",
    "0.1833565231089554\n",
    ">>> # Calculate dy/dx at x1 = 5, x2 = 3\n",
    ">>> g.der\n",
    "array([0.17582502, -0.05175055])\n",
    ">>> # Create a reverse mode instance.\n",
    ">>> rev = Reverse(f, *x)\n",
    ">>> # Evaluate the value of y at x1 = 5, x2 = 3\n",
    ">>> rev.val\n",
    "0.1833565231089554\n",
    ">>> # Calculate dy/dx at x1 = 5, x2 = 3\n",
    ">>> rev.der\n",
    "array([0.17582502, -0.05175055])\n",
    "```\n",
    "\n",
    "#### **3.2.3 Example 3: univariate vector function**\n",
    "```python\n",
    ">>> from AutoDiff import Forward, Reverse\n",
    ">>> import numpy as np\n",
    ">>> x = 50\n",
    ">>> def f(x):\n",
    ">>>     return [np.sin(x), np.cos(x)]\n",
    ">>> g = Forward(f, x)\n",
    ">>> g.val\n",
    "array([-0.26237485,  0.96496603])\n",
    ">>> g.der\n",
    "array([0.96496603, 0.26237485])\n",
    ">>> rev = Reverse(f, x)\n",
    ">>> rev.val\n",
    "array([-0.26237485,  0.96496603])\n",
    ">>> rev.der\n",
    "array([[0.96496603, 0.26237485]])\n",
    "```\n",
    "\n",
    "#### **3.2.4 Example 4: multivariate vector function**\n",
    "```python\n",
    ">>> from AutoDiff import Forward, Reverse\n",
    ">>> import numpy as np\n",
    ">>> x = [5, 3]\n",
    ">>> def f(x1, x2):\n",
    ">>>     return [np.exp(np.cos(x1)+2*np.cos(x2)), x1 * x2, 2]\n",
    ">>> g = Forward(f, *x)\n",
    ">>> g.val\n",
    "array([ 0.18335652, 15.        ,  2.        ])\n",
    ">>> g.der\n",
    "array([[ 0.17582502, -0.05175055],\n",
    "       [ 3.        ,  5.        ],\n",
    "       [ 0.        ,  0.        ]])\n",
    ">>> rev = Reverse(f, *x)\n",
    ">>> rev.val\n",
    "array([ 0.18335652, 15.        ,  2.        ])\n",
    ">>> rev.der\n",
    "array([[ 0.17582502, -0.05175055],\n",
    "       [ 3.        ,  5.        ],\n",
    "       [ 0.        ,  0.        ]])\n",
    "```\n",
    "#### **3.2.5 Example 5: Newton's Method**\n",
    "\n",
    "Assume we want to compute $\\sqrt 2$, that is, finding $x$ such that $f(x) \\triangleq x^2 - 2 = 0$. Let's begin with a random guess $x_0 = 1.4$. Newton method essentially compute\n",
    "\n",
    "$$x_i = x_{i-1} - \\frac{f(x_{i-1})}{f^{\\prime}(x_{i-1})}$$\n",
    "\n",
    "```python\n",
    ">>> from AutoDiff import Forward\n",
    ">>> import numpy as np\n",
    ">>> f = lambda x: x**2 - 2\n",
    ">>> x0 = 1.4\n",
    ">>> def newton(f, x0, tol=1e-10):\n",
    "        if abs(f(x0)) < tol:\n",
    "            return x0\n",
    "        g = Forward(f, x0)\n",
    "        new_x0 = x0 - g.val / g.der\n",
    "        return newton(f, new_x0)\n",
    ">>> our_sol = newton(f, x0)\n",
    ">>> our_sol\n",
    "array([1.41421356])\n",
    ">>> assert np.allclose(np.sqrt(2), our_sol)\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.0 Software Organization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section briefly discusses our plan on organizing the Python package `AutoDiff`. The directory structure is shown below, following the [recommended python package structure](https://packaging.python.org/en/latest/tutorials/packaging-projects/).\n",
    "\n",
    "```\n",
    ". # root dir\n",
    "├── .github/workflows # this folder contains all Github workflow actions yml files\n",
    "│   ├── coverage.yml\n",
    "│   ├── sphinx_build.yml\n",
    "│   └── test.yml\n",
    "├── docs # development documentations\n",
    "│   ├── documentation.pdf\n",
    "│   ├── milestone1.pdf\n",
    "│   ├── milestone2_progress.md\n",
    "│   └── milestone2.pdf\n",
    "│   ├── src # this folder holds all the source code for milestone report\n",
    "│   │   ├── documentation.ipynb\n",
    "│   │   ├── milestone1.ipynb\n",
    "│   │   └── milestone2.ipynb\n",
    "│   └── source # autogenerated docs by sphinx\n",
    "│       └── Makefile\n",
    "├── example # examples for how to use this package\n",
    "│   ├── forward_mode.py\n",
    "│   ├── newton.py\n",
    "│   ├── plot_graph.py\n",
    "│   ├── reverse_mode.py\n",
    "│   └── sgd.py\n",
    "├── LICENSE\n",
    "├── README.md\n",
    "├── Dockerfile\n",
    "├── setup.py\n",
    "├── requirements.txt\n",
    "├── test\n",
    "│   ├── __init__.py\n",
    "│   ├── run_coverage.sh # shell script that print out the coverage report for code base\n",
    "│   ├── run_test.sh # shell script that run all tests for code base\n",
    "│   ├── test_node.py # test for node module\n",
    "│   ├── test_forward.py # test for forward module\n",
    "│   ├── test_optimization.py # test for optimization modules\n",
    "│   ├── test_plot_computation_graph.py # test for plot_computation_graph module\n",
    "│   └── test_rnode.py # test for rnode module\n",
    "│   └── test_reverse.py # test for reverse module\n",
    "└── AutoDiff # our library source code\n",
    "    ├── graphvis # directory for computation graph visualization additional feature\n",
    "    │   ├── __init__.py\n",
    "    │   ├── computationGraph # automatically generated file from graphviz (an example)\n",
    "    │   ├── computationGraph.png # automatically generated computation graph image (an example)\n",
    "    │   └── plot_computation_graph.py # plot_computation_graph module\n",
    "    ├── optim # directory for optimization additional feature\n",
    "    │   ├── __init__.py\n",
    "    │   ├── newton.py # Newton's method module\n",
    "    │   └── sgd.py # SGD module\n",
    "    ├── __init__.py\n",
    "    ├── node.py # node module\n",
    "    ├── forward.py # forward module\n",
    "    ├── rnode.py # rnode module\n",
    "    └── reverse.py # reverse module\n",
    "```\n",
    "\n",
    "The functionalities of each aforementioned directory/file is shown below.\n",
    "\n",
    "- `.github/workflows`: This directory contains files for github actions for continuous integration\n",
    "\n",
    "- `docs`: This directory contains the major deliverables from each milestone.\n",
    "\n",
    "- `docs/source`: We use [sphinx](https://www.sphinx-doc.org/en/master/) for automated documentation generation due to its popularity. Such flexible automated tool read content from the docstring and create documentations automatically.\n",
    "\n",
    "- `example`: This directory provides examples to show users how to use the package.\n",
    "\n",
    "- `LICENSE`: This is the file specifying the license for our python package.\n",
    "\n",
    "- `README.md`: This is the readme for the project repository.\n",
    "\n",
    "- `Dockerfile`: This is the docker file that contains the configurations of our docker image.\n",
    "\n",
    "- `setup.py`: This is used for defining and publishing our package. It contains the version number, dependencies, upstream GitHub URL, and etc. The functionality is described in [here](https://docs.python.org/3/distutils/setupscript.html).\n",
    "\n",
    "- `requirements.txt`: This is the basic requirements needed for user to use our package.\n",
    "\n",
    "- `test`: This is the directory that contains the [pytest](https://docs.pytest.org/en/7.1.x/) modules. They are unit tests and these tests are separated into modules. Two shell scripts are provided, all tests in the `test` folder will be run by executing `run_test.sh`, and a coverage report will be produced by executing `run_coverage.sh`.\n",
    "\n",
    "- `AutoDiff`: This is the folder for the actual source code. We follow the recommended structure of python module shown in [the python documentation](https://docs.python.org/3/tutorial/modules.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clearly visualize the directory structure, we did not include files such as `.gitignore`, `.github` for github action, `.vscode/settings.json` file for PyTest on Visual Studio Code,and `requirements.txt` for listing dependencies in local development. \n",
    "\n",
    "For distribution, we package the code by the standard [setuptools](https://setuptools.pypa.io/en/latest/) and then distribute our software using [Python Package Index (PyPI)](https://pypi.org/). In this way, users can download the package by simply using the following command.\n",
    "\n",
    "```bash\n",
    "pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple AutoDiff-Team01\n",
    "```\n",
    "\n",
    "Tests for the source code in `AutoDiff` package are stored in the `test` directory. These are `pytest` tests. Since the development of the `AutoDiff` package is done using Visual Studio Code (VSCode), the testing functionality in VSCode is leveraged by specifying the location of the tests and the type of tests (i.e., `pytest`) in the `settings.json` file in the `.vscode` directory. Developers can go into the \"Testing\" tab in VSCode and run any test of interest. VSCode also creates a button besides all tests in the `test` folder, so that all tests can be run individually. \n",
    "\n",
    "In addition, GitHub action is used to conduct continuous integration (CI), including running `pytest` tests. Each time a commit is pushed to GitHub, `pytest` tests are initiated. Specifically, in the `.github/workflows` directory, the `test.yml` file calls the `run_test.sh` shell script and the list of test cases specified in `tests` are run. The `coverage.yml` file checks the test coverage for the source code in the `AutoDiff` package by calling the `run_coverage.sh` shell script and a test coverage report will be generated. Notice that there is another file named `sphinx_build.yml`. This is for automatically generating documentation for the `AutoDiff` package. Locally, `run_test.sh` and `run_coverage.sh` are the two shell scripts that can be edited and run to conduct testing on the source code and check test coverage. \n",
    "\n",
    "Finally, the traditional Git workflow is followed. The main branch is used for stable release of the package and all development and testinf is conducted in separate branches. Developers only merge to main branch if the code in development branch passes the test. The test coverage report is generated by `pytest` and published on the GitHub page [here](https://code.harvard.edu/pages/CS107/team01/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.0 Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `AutoDiff` package currently contains two classes: `Node` and `Forward`. Our implementation currently needs to depend on the NumPy library.\n",
    "\n",
    "Before computing the gradient of the input function, the input function is wrapped into a graph structure that stores the partial ordering of the intermediate results $v_j$ (Section 2.3). To realize the graph structure, we need to implement the class `Node`, where each node contains a reference of its parents. Since we have constructed the core data structure `Node` of the input function, we can implement the forward mode of automatic differentiation in the class `Forward` to compute the Jacobian of the input function. The following subsections are more detailed introduction of the classes with class attributes and methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1  `class Node`**\n",
    "The class `Node` turns the function into the core data structure -- graph. Attributes of `Node` includes `self.val`, `self.der`, `self.parent`, `self.op`, and `self.v_index`. The class `Node` incorporate the structure of dual numbers (Section 2.3.5), where `self.val` + `self.der` $\\epsilon$ is assemble to the dual number's structure $a + b \\epsilon$. In this class, we further overload the elementary operators and useful numpy functions. \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    This is a class that implements the Dual numbers and dunder methods to overload \n",
    "    built-in operators including negation, addition, subtraction, multiplication, true\n",
    "    division, and power. The class also overloads numpy operators including square root,\n",
    "    exponential, logarithm, sine, cosine, and tangent. Reflective operations are also \n",
    "    included.\n",
    "\n",
    "    :param val: The value of the Node, it can be an interger or float or a 1D numpy array \n",
    "    for 1D problem, or a multi-dimensional numpy array for multi-dimensional problem\n",
    "    :type val: integer or float or numpy array\n",
    "    :param der: The derivative of the Node, defaults to 1. It can be an interger or float \n",
    "    or a 1D numpy array for 1D problem, or a multi-dimensional numpy array for \n",
    "    multi-dimensional problem\n",
    "    :type der: integer or float or numpy array\n",
    "    :param parent: A list of parent Nodes of the current Node\n",
    "    :type parent: list\n",
    "    :param op: A list of strings representing operations\n",
    "    :type op: list\n",
    "    \"\"\"\n",
    "\n",
    "    _supported_types = (int, float)\n",
    "    v_index = 0\n",
    "\n",
    "    def __init__(self, value, derivative=1): \n",
    "        \"\"\"Constructor method\n",
    "        \"\"\"\n",
    "        self.val = value\n",
    "        self.der = derivative\n",
    "        self.parent = []\n",
    "        self.op = []\n",
    "        type(self).v_index += 1\n",
    "        self.v_index = f'v{type(self).v_index}'\n",
    "\n",
    "    def update_node(self, parent, op):\n",
    "        \"\"\"Update a list of parent Nodes of the current Node with their operations.\n",
    "        :param parent: A list of parent Nodes of the current Node\n",
    "        :type parent: list\n",
    "        :param op: A list of strings representing operations\n",
    "        :type op: list\n",
    "        \n",
    "        :return: Returns the current Node with parents and operations updated\n",
    "        :rtype: Node\n",
    "        \"\"\"\n",
    "        self.parent = parent\n",
    "        self.op = op\n",
    "        return self\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Node: vindex={self.v_index}, val={self.val}, der={self.der}, parent={self.parent}, and op={self.op}.'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'A Node object with index of {self.v_index}, value of {self.val}, derivative of {self.der}, parent of {self.parent}, and operator of {self.op}.'\n",
    "\n",
    "    \"\"\"\n",
    "    Some examples of elementary operators that are overloaded\n",
    "    \"\"\"\n",
    "    def __neg__(self):\n",
    "        value = -self.val\n",
    "        derivative = -1 * self.der\n",
    "        return Node(value, derivative).update_node([self], ['-1*'])\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Node):\n",
    "            value = self.val + other.val\n",
    "            derivative = self.der + other.der\n",
    "            return Node(value, derivative).update_node([self, other], ['+'])\n",
    "        elif not isinstance(other, self._supported_types):\n",
    "            raise TypeError(f\"Type `{type(other)}` is not supported for addition\")\n",
    "        else:\n",
    "            value = self.val + other\n",
    "            derivative = self.der\n",
    "            return Node(value, derivative).update_node([self], ['+', other])\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, Node):\n",
    "            value = self.val - other.val\n",
    "            derivative = self.der - other.der\n",
    "            return Node(value, derivative).update_node([self, other], ['-'])\n",
    "        elif not isinstance(other, self._supported_types):\n",
    "            raise TypeError(f\"Type `{type(other)}` is not supported for subtraction\")\n",
    "        else:\n",
    "            value = self.val - other\n",
    "            derivative = self.der\n",
    "            return Node(value, derivative).update_node([self], ['-', other])\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, Node):\n",
    "            value = self.val * other.val\n",
    "            derivative = self.der * other.val + other.der * self.val\n",
    "            return Node(value, derivative).update_node([self, other], ['*'])\n",
    "        elif not isinstance(other, self._supported_types):\n",
    "            raise TypeError(f\"Type `{type(other)}` is not supported for multiplication\")\n",
    "        else:\n",
    "            value = self.val * other\n",
    "            derivative = self.der * other\n",
    "            return Node(value, derivative).update_node([self], ['*', other])\n",
    "\n",
    "    def sqrt(self):\n",
    "        if self.val < 0:\n",
    "            raise ValueError('Cannot take square root of negative number.')\n",
    "        value = np.sqrt(self.val)\n",
    "        derivative = 0.5/np.sqrt(self.val) * self.der\n",
    "        return Node(value, derivative).update_node([self], ['sqrt()'])\n",
    "\n",
    "    def log(self, base=np.e):\n",
    "        if self.val <= 0:\n",
    "            raise ValueError('Cannot take the log of a negative number.')\n",
    "        if base == np.e:\n",
    "            value = np.log(self.val)\n",
    "            derivative = 1 / self.val * self.der\n",
    "            return Node(value, derivative).update_node([self], ['log()'])\n",
    "        else:\n",
    "            value = np.log(self.val) / np.log(base)\n",
    "            derivative = 1 / (self.val * np.log(base)) * self.der\n",
    "            return Node(value, derivative).update_node([self], [f'log{base}()'])\n",
    "\n",
    "    \"\"\"\n",
    "    Other elementory operators and reflective operators that are also overloaded\n",
    "    \"\"\"\n",
    "    logistic(self)\n",
    "    __truediv__(self, other)\n",
    "    __pow__(self, other)\n",
    "    exp(self)\n",
    "    sin(self)\n",
    "    cos(self)\n",
    "    tan(self)\n",
    "    arcsin(self)\n",
    "    arccos(self)\n",
    "    arctan(self)\n",
    "    sinh(self)\n",
    "    cosh(self)\n",
    "    tanh(self)\n",
    "    __radd__(self, other)\n",
    "    __rsub__(self, other)\n",
    "    __rmul__(self, other)\n",
    "    __rtruediv__(self, other)\n",
    "    __rpow__(self, other)\n",
    "    __lt__(self, other)\n",
    "    __gt__(self, other)\n",
    "    __le__(self, other)\n",
    "    __ge__(self, other)\n",
    "    __eq__(self, other)\n",
    "    __ne__(self, other)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2 `class Forward`**\n",
    "In the class `Forward`, we need to model arbitrary high-level function $f$. We treat a vector function $f\\colon\\mathbb{R}^m\\mapsto\\mathbb{R}^n$ as a list of scalar functions $f\\colon\\mathbb{R}^m\\mapsto\\mathbb{R}$. Our key observation is that, once the number of input variables are known, we can iterate over all natural basis and can obtain the jacobian in one pass.\n",
    "\n",
    "Specifically, the `grad` method below ompute the full Jacobian by looping through the vector of scalar functions.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from .node import Node\n",
    "class Forward:\n",
    "    \"\"\"\n",
    "    Forward mode AD for vector functions of vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, f: callable, *variables):\n",
    "        \"\"\"\n",
    "        Initialize forward class. For detailed implementation see :py:meth:`AutoDiff.forward.Forward.grad`.\n",
    "\n",
    "        Args:\n",
    "            f: Function that is callable\n",
    "\n",
    "            variables: inputs\n",
    "        \"\"\"\n",
    "        self.val, self.der, self.output = self.grad(f, *variables)\n",
    "\n",
    "    @staticmethod\n",
    "    def grad(f: callable, *variables):\n",
    "        r\"\"\"\n",
    "        Evaluate the full Jacobian in forward mode. This is the method that is used internally\n",
    "        by :py:meth:`AutoDiff.forward.Forward.__init__`.\n",
    "        For each (scalar or multivariate) function ``f``,\n",
    "        use :math:`m` passes with different seed vector :math:`\\mathbf{e}`,\n",
    "        where each natural basis :math:`\\mathbf{e} \\in \\mathbb{R}^{m}`, and :math:`m` is the number in ``variables``.\n",
    "\n",
    "        :param f: A callable function object to perform differentiation on\n",
    "        :type f: function object\n",
    "        :param variables: The input for variables of function ``f``\n",
    "        :type variables: integer or float or numpy array or list of intergers or floats\n",
    "\n",
    "        :return: function evaluation at variable x, Jacobian, a single output node (scalar function) or a list of output nodes (multivariate) function)\n",
    "            Stack the gradient rows into the full Jacobian.\n",
    "        :rtype: Node class object or list of Node class objects        \n",
    "        \"\"\"\n",
    "        # Helper function when Forward class is initialized, see usage in init.\n",
    "        num_variables = len(variables)\n",
    "        # initialize the intermediate result index\n",
    "        Node.v_index = -num_variables\n",
    "        # Convert variables into Nodes and store in a list\n",
    "        variables = [\n",
    "            Node(var, derivative = np.eye(num_variables)[i])\n",
    "            for i, var in enumerate(variables)\n",
    "        ]\n",
    "        # Perform the forward mode\n",
    "        output = f(*variables)\n",
    "        if isinstance(output, list): # for vector functions (a list of outputs)\n",
    "            output = [o if isinstance(o, Node) else Node(o, np.zeros(num_variables)) for o in output]\n",
    "            values = np.array([o.val for o in output])\n",
    "            ders = np.stack([o.der if len(o.der) > 1 else o.der[0] for o in output])\n",
    "            return values, ders, output\n",
    "        else: # for scalar functions (a single output)\n",
    "            if not isinstance(output, Node):\n",
    "                output = Node(output, np.zeros(num_variables))\n",
    "            return output.val, output.der if len(output.der) > 1 else output.der[0], output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Additional Features and Extensions**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional features and extensions, the Reverse Mode of Automatic Differentiation is achieved by two new classes: `class RNode` and `class Reverse`. We also extend the package for various optimizers as well as creating computational graph visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.1 Reverse Mode**\n",
    "#### **6.1.1 `class RNode`**\n",
    "In the class `RNode`, we implement the basic Reverse Node class for performing Reverse mode. The parameters `self.val` stores the function value and calculates the Forward Pass. The parameter `self.parent` stores a list of tuples, containing the partial derivative of parent with respect to self and the parent of the node. We then calculate the gradient of the function and store it in `self.der`. We achieve the multivariable functionality in class `Reverse`.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "class RNode:\n",
    "    \"\"\"This is a class that implements the Reverse Nodes for reverse mode calculation of\n",
    "    automatic differentiation and dunder methods to overload built-in operators including \n",
    "    negation, addition, subtraction, multiplication, true division, and power. The class \n",
    "    also overloads numpy operators including square root, exponential, logarithm, sine, \n",
    "    cosine, and tangent. Reflective operations are also included.\n",
    "\n",
    "    :param val: The value of the RNode, it can be an interger or float or a 1D numpy array \n",
    "    for 1D problem, or a multi-dimensional numpy array for multi-dimensional problem\n",
    "    :type val: integer or float or numpy array\n",
    "    :param der: The parent of the RNode, defaults to None. It can be an interger or float \n",
    "    or a 1D numpy array for 1D problem, or a multi-dimensional numpy array for \n",
    "    multi-dimensional problem\n",
    "    :type der: integer or float or numpy array\n",
    "    :param parent: A list of parent RNodes of the current RNode\n",
    "    :type parent: list\n",
    "    \"\"\"\n",
    "    def __init__(self, val):\n",
    "        \"\"\"Constructor method\n",
    "        \"\"\"\n",
    "        self.val = val\n",
    "        self.der = None\n",
    "        self.parent = []\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clears self's and all self's parent's derivative field\n",
    "        \"\"\"\n",
    "        self.der = None\n",
    "        for _, p in self.parent:\n",
    "            p.clear()\n",
    "\n",
    "    def grad_vec(self, output_depend):\n",
    "        \"\"\"Helper function for Reverse AD, produces all gradient for its parents \n",
    "        and any parents defined in the intermediate step. See usage in Reverse class.\n",
    "        \"\"\"\n",
    "        gradient = []\n",
    "        for i in range(len(self.parent)):\n",
    "            gradient.append(self.parent[i][0] * self.parent[i][1].grad(output_depend))\n",
    "        return gradient\n",
    "\n",
    "    def grad(self, output_depend):\n",
    "        \"\"\"Helper function for grad_vec(), see usage in grad().\n",
    "        \"\"\"\n",
    "        if self.parent == []:\n",
    "            self.der = 1.0\n",
    "            output_depend.append(self)\n",
    "        else:\n",
    "            self.der = sum(p[0] * p[1].grad(output_depend) for p in self.parent)\n",
    "        return self.der\n",
    "    \n",
    "    def __neg__(self):\n",
    "        rnode = RNode(-self.val)\n",
    "        self.parent.append((-1, rnode))\n",
    "        return rnode\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, RNode):\n",
    "            rnode = RNode(self.val + other.val)\n",
    "            self.parent.append((1., rnode))\n",
    "            other.parent.append((1., rnode))\n",
    "            return rnode\n",
    "        elif not isinstance(other, self._supported_types):\n",
    "            raise TypeError(f\"Type `{type(other)}` is not supported for addition\")\n",
    "        else:\n",
    "            rnode = RNode(self.val + other)\n",
    "            self.parent.append((1., rnode))\n",
    "            return rnode\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, RNode):\n",
    "            rnode = RNode(self.val - other.val)\n",
    "            self.parent.append((1., rnode))\n",
    "            other.parent.append((-1., rnode))\n",
    "            return rnode\n",
    "        elif not isinstance(other, self._supported_types):\n",
    "            raise TypeError(f\"Type `{type(other)}` is not supported for subtraction\")\n",
    "        else:\n",
    "            rnode = RNode(self.val - other)\n",
    "            self.parent.append((1., rnode))\n",
    "            return rnode\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, RNode):\n",
    "            rnode = RNode(self.val * other.val)\n",
    "            self.parent.append((other.val, rnode))\n",
    "            other.parent.append((self.val, rnode))\n",
    "            return rnode\n",
    "        elif not isinstance(other, self._supported_types):\n",
    "            raise TypeError(f\"Type `{type(other)}` is not supported for multiplication\")\n",
    "        else:\n",
    "            rnode = RNode(self.val * other)\n",
    "            self.parent.append((other, rnode))\n",
    "            return rnode\n",
    "\n",
    "    \"\"\"\n",
    "    Other elementory operators and reflective operators also need to be overloaded\n",
    "    \"\"\"\n",
    "    __str__(self)\n",
    "    __repr__(self)\n",
    "    __truediv__(self, other)\n",
    "    __pow__(self, other)    \n",
    "    sqrt(self)\n",
    "    log(self)\n",
    "    logistic(self)\n",
    "    exp(self)\n",
    "    sin(self)\n",
    "    cos(self)\n",
    "    tan(self)\n",
    "    arcsin(self)\n",
    "    arccos(self)\n",
    "    arctan(self)\n",
    "    sinh(self)\n",
    "    cosh(self)\n",
    "    tanh(self)\n",
    "    __radd__(self, other)\n",
    "    __rsub__(self, other)\n",
    "    __rmul__(self, other)\n",
    "    __rtruediv__(self, other)\n",
    "    __rpow__(self, other)\n",
    "    __lt__(self, other)\n",
    "    __gt__(self, other)\n",
    "    __le__(self, other)\n",
    "    __ge__(self, other)\n",
    "    __eq__(self, other)\n",
    "    __ne__(self, other)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6.1.2 `class Reverse`**\n",
    "\n",
    "In the class `Reverse`, we perform the reverse mode automatic differentiation. The `grad()` method evaluates the function at user-given values of x and computes the full Jacobian at x, notice that by our implementation of the `RNode` class, we get the partial derivatives column in forward pass of the reverse mode computation table while initializing the function.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from .rnode import RNode\n",
    "class Reverse:\n",
    "    def __init__(self, f: callable, *variables):\n",
    "        \"\"\"\n",
    "        Initialize reverse class. For detailed implementation see :py:meth:`AutoDiff.reverse.Reverse.grad`.\n",
    "\n",
    "        Args:\n",
    "            f: Function that is callable\n",
    "\n",
    "            variables: inputs\n",
    "        \"\"\"\n",
    "        self.val, self.der, self.output = self.grad(f, *variables) \n",
    "\n",
    "    @staticmethod\n",
    "    def grad(f: callable, *variables):\n",
    "        r\"\"\"\n",
    "        Evaluate the full Jacobian in reverse mode. This is the method that is used internally\n",
    "        by :py:meth:`AutoDiff.reverse.Reverse.__init__`.\n",
    "        For each scalar function, use a forward pass and a reverse pass.\n",
    "        Stack the partial derivative columns into the full Jacobian.\n",
    "\n",
    "        :param f: A callable function to perform differentaition on\n",
    "        :type f: function object\n",
    "\n",
    "        :param variables: The input for variables of function ``f``\n",
    "        :type variables: integer or float or numpy array or list of integers or floats\n",
    "\n",
    "        :return: Jacobian\n",
    "            Stack the partial derivative columns into the full Jacobian.\n",
    "        :rtype: integer or float or numpy array\n",
    "        \"\"\"\n",
    "        # interate through variables and stack the partial derivative columns into Jacobian\n",
    "        for var in variables:\n",
    "            output_depend = []\n",
    "            var_der = var.grad_vec(output_depend)\n",
    "            # var_der: ordered list of derivatives of each parent of var\n",
    "            # output_depend: ordered list of output rnode that each parent of var points to\n",
    "            var.clear() # clear the paths to prepare for the next iteration of variables\n",
    "            # sum up the partial derivaties of each scalar function in var_der\n",
    "            var_der = df/dvar # details will not be shown here\n",
    "            ders.append(var_der)\n",
    "            \n",
    "        ders = np.stack(ders, axis = -1) # stack derivatives of each var\n",
    "        if isinstance(output, list): # vector function\n",
    "            values = np.array([o.val for o in output])\n",
    "            if num_variables == 1: \n",
    "                ders = ders.T # reshape column vector to 1d array\n",
    "        else: # scalar function\n",
    "            values = output.val\n",
    "            ders = ders.T\n",
    "            if len(ders) == 1:\n",
    "                ders = ders[0]\n",
    "        return values, ders\n",
    "        \n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2 Optimization**\n",
    "\n",
    "We provide optimizers including Netwon's method and Stochastic Gradient Descent.\n",
    "\n",
    "- Newton's Method iteratively updates $x$ using the following equation:\n",
    "$$\n",
    "x_{k+1} \\gets x_k - f(x_k) / f'(x_k)\n",
    "$$\n",
    "\n",
    "\n",
    "- Stochastic Gradient Descent (SGD) iteratively updates $x$ using the following equation:\n",
    "\n",
    "$$\n",
    "x_{k+1} \\gets x_k - \\eta \\nabla_x f(x_k)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6.2.1 Newton's Method**\n",
    "```python\n",
    "import numpy as np\n",
    "from .. import Forward\n",
    "def Newton(f: callable, *x0, tol=1e-5, max_iter=1000, n_iter=1):\n",
    "    r\"\"\"\n",
    "    Newton's method\n",
    "\n",
    "    To find :math:`\\mathbf{x}` such that :math:`F(\\mathbf{x}) = \\mathbf{0}`, we need update :math:`\\Delta \\mathbf{x}_{k}` such that\n",
    "\n",
    "    .. math::\n",
    "                  J_F(\\mathbf{x}_k)\\Delta \\mathbf{x}_{k} &= - F(\\mathbf{x}_{k}) \\\\\n",
    "              \\mathbf{x}_{k+1} &\\gets \\mathbf{x}_{k} + \\Delta \\mathbf{x}_{k}\n",
    "    where :math:`J_F(\\mathbf{x}_k)` is the Jacobian and :math:`\\Delta \\mathbf{x}_{k}` is the update.\n",
    "    \n",
    "    :param f: callable, the :math:`F: \\mathbb{R}^m \\mapsto \\mathbb{R}^n` function\n",
    "    :param x0: initial guess, note that Newton is quadratic convergence if initial guess is close to the actual solution\n",
    "    :param tol: tolerance, the algorithm terminates when it hits the tolerance i.e. when :math:`\\|F(\\mathbf{x})\\|_F < \\text{tol}` is reached, the algorithm terminates\n",
    "    :return: the solution :math:`\\mathbf{x}`\n",
    "    \"\"\"\n",
    "    n_iter += 1\n",
    "    if n_iter == max_iter:\n",
    "        raise RuntimeError(f'The function does not converge in {n_iter} iterations!')\n",
    "    x0 = np.array(x0)\n",
    "    # if the norm of the function is less than the tolerance, consider the method converged\n",
    "    if np.linalg.norm(f(*x0)) < tol:\n",
    "        # if result list has length 1, return the number without the bracket\n",
    "        if len(x0) == 1:\n",
    "            return x0.item()\n",
    "        return x0\n",
    "    # use Forward AD for derivative calculation\n",
    "    g = Forward(f, *x0)\n",
    "    if isinstance(g.der, (int, float)):\n",
    "        # if the derivative g is a number, perform Newton's Method in 1D\n",
    "        new_x = x0 - g.val / g.der\n",
    "    else:\n",
    "        # else perform Newton's Method in nD\n",
    "        if g.der.ndim == 1:\n",
    "            g.der = g.der.reshape(1, -1)\n",
    "            g.val = np.array(g.val).reshape(1)\n",
    "        update, *_ = np.linalg.lstsq(g.der, -g.val, rcond=None)\n",
    "        new_x = x0 + update\n",
    "    return Newton(f, *new_x, tol=tol, n_iter=n_iter)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6.2.2 Stochastic Gradient Descent**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from .. import Forward\n",
    "def SGD(f: callable, *x0, eta=1e-1, n_iter=50000, tol=1e-5):\n",
    "    r\"\"\"\n",
    "    Stochastic gradient descent\n",
    "\n",
    "    It optimizes the following procedure iteratively\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x} \\gets \\mathbf{x} - \\eta \\nabla f(\\mathbf{x})\n",
    "\n",
    "    where :math:`f: \\mathbb{R}^n \\mapsto \\mathbb{R}`\n",
    "\n",
    "    :param f: callable, the :math:`F: \\mathbb{R}^n \\mapsto \\mathbb{R}` function\n",
    "    :param x0: initial guess\n",
    "    :param eta: learning rate :math:`\\eta`, which needed to be picked for each specific optimization task\n",
    "    :param n_iter: after :code:`n_iter` steps the algorithm will terminate\n",
    "    :param tol: the algorithm terminates when it reaches the tolerance, i.e. when :math:`|f(\\mathbf{x})| < \\text{tol}` is reached\n",
    "    :raises RuntimeError: The function does not converge in n_iter iterations!\n",
    "    :return: the final solution\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while i < n_iter:\n",
    "        # if the norm of the function is less than tolerance, consider the method converged\n",
    "        if np.linalg.norm(f(*x0)) < tol:\n",
    "            # if result list has length 1, return the number without the bracket\n",
    "            if len(x0) == 1:\n",
    "                return x0.item()\n",
    "            return x0\n",
    "        i += 1\n",
    "        # use Forward AD for derivative calculation\n",
    "        g = Forward(f, *x0)\n",
    "        # apply stochastic gradient descent\n",
    "        x0 -= eta * g.der\n",
    "    # if the function does not converge in 50000 iterations, we consider the function does not converge, can raise a Runtime error\n",
    "    raise RuntimeError(f'The function does not converge in {n_iter} iterations!')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.3 Computational Graph Visualization**\n",
    "\n",
    "The `Graphviz` software is used to support the implementation of computation graph visualization. The computation graph is developed by taking the output from the forward mode and recursively iterating through each node’s parent and operation until there is no more parent. As we traverse through the parents and operations, we add nodes and edges to the graph. Each node represents a variable or an intermediate result. Each edge specifies the from node and the to node along with the operation that the edge takes on. Two functions are used to implement the visualization functionality: `build_graph(g, output)` and `generate_graph(x, g)`. `build_graph(g, output)` is the helper function that adds all nodes and edges into a graphviz Graph object. `generate_graph(x, g)` is the main visualization function that ties everything together to produce a visualization of the computation graph and then outputs the final computation graph as a png file. For example, given the input function\n",
    "\n",
    "$$f(x_1,x_2,x_3)\n",
    "= \\begin{bmatrix} sin(x_1)-x_2-x_3 \\\\ cos(x_2) \\times x_1 / x_3 \\end{bmatrix}$$\n",
    "\n",
    "After completing the forward mode, `generate_graph(x, g)` can be called to produce the following computation graph visualization.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1e7S7rX31m61s9GUYgYA6gC-jTNA_-TD8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.0 Broader Impact and Inclusivity Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The potential broader impacts and implications of `AutoDiff` could be significant, as it has the potential to make it easier for researchers and developers to implement automatic differentiation in their own projects. Automatic differentiation is a powerful tool for optimizing machine learning algorithms, which can have a wide range of applications in various fields. \n",
    "\n",
    "One potential way that people could use `AutoDiff` responsibly is by using it to improve the performance of machine learning algorithms in a way that is transparent and explainable. For example, if `AutoDiff` is used to optimize a predictive model in healthcare, finance, or environmental science, the results of the optimization should be clearly communicated and explained to clinicians and patients or other relevant stakeholders, so that they can understand the model's predictions and make informed decisions.\n",
    "\n",
    "On the other hand, there could be potential ethical implications if `AutoDiff` is used irresponsibly or without proper oversight. For example, if a predictive model optimized with `AutoDiff` is used to make decisions that have a significant impact on people's lives, such as in hiring or loan applications, it is important that the model is fair and unbiased. If the model is not properly validated or checked for bias, it could have negative consequences for the individuals who are affected by its decisions. \n",
    "\n",
    "Overall, it is important for users of AutoDiff to use the software responsibly and take into account the potential broader impacts and ethical implications of their work. By doing so, they can help to ensure that the technology is used for the benefit of society, rather than causing harm.\n",
    "\n",
    "In terms of inclusivity, one potential way that `AutoDiff` could be inclusive to the broader community is by making automatic differentiation more accessible to a wider range of users. Currently, automatic differentiation can be difficult to implement, especially for users who do not have a strong background in mathematics or computer science. By providing an easy-to-use software package, `AutoDiff` could make automatic differentiation more accessible to a broader range of users, including those from underrepresented groups. This could help to promote diversity and inclusion in the fields of machine learning and artificial intelligence.\n",
    "\n",
    "Additionally, `AutoDiff` could be inclusive by providing documentation and user support in multiple languages, which would make it more accessible to users who speak languages other than English. In the future, we plan to support more languages such as Chinese and Spanish. This could be especially useful for users in non-English speaking countries, where access to resources and support for machine learning and artificial intelligence can be more limited. By providing support in multiple languages, `AutoDiff` could help to break down language barriers and make it easier for users from diverse backgrounds to access and use the software.\n",
    "\n",
    "For any developers who would like to build on top of the `AutoDiff` package, they can fork the `AutoDiff` GitHub repository. If they find an issue with the implementation of the `AutoDiff` package or have any suggestions for improvements, they can send a pull request from the forked GitHub respository. All developers of the `AutoDiff` package (i.e., our AC207 project team) will review the pull request and decide whether or not to integrate the code change into the `AutoDiff` package source code. Once all team members review the pull request and confirm that there are no more issues, one of the team members will approve the pull request and merge the pull request into the `main` branch for `AutoDiff`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8.0 Future Work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic Differentiation can be useful in many fields.\n",
    "\n",
    "In physics, this can be useful for a number of applications. For example, it can be used to quickly and accurately compute the derivatives of functions that describe physical phenomena, such as the motion of a particle or the evolution of a physical system over time. This can allow physicists to more easily solve complex problems, and to make more precise predictions about the behavior of physical systems. Additionally, auto differentiation can be used to optimize physical models, by adjusting model parameters in such a way as to best fit experimental data.\n",
    "\n",
    "In the process of gene expression, where genes are transcribed into RNA and then translated into proteins, can be modeled using differential equations. By using automatic differentiation, researchers can quickly and accurately compute the derivatives of these equations, which can provide important insights into the underlying mechanisms of gene expression. Additionally, automatic differentiation can be used to train machine learning models that can be used for tasks such as predicting the effects of genetic mutations or identifying patterns in large genomic datasets.\n",
    "\n",
    "Automatic Differentiation can also be widely used in applied mathematics field. For example, in fluid dynamics, the gradient of the water flow needs to be calculated in order to analyze velocities and pressures in a 3D space. The application of fluid dynamics is widely spreaded from analyzing dynamics for aircrafts to determing the flow rate through water pipelines.\n",
    "\n",
    "In the field of public health, automatic differentiation can be used in the study of the diffusion model. For example, we can trace the spreading speed of the virus. This would be helpful for the scientists to generate valid and practical advice for the government in the process of establishing public health policies.\n",
    "\n",
    "While many exciting projects can be built on top of our `AutoDiff` package, there are still many additional features that we can improve in the package itself.\n",
    "\n",
    "The first limitation is that the current Reverse Mode implementation for automatic differentiation does not support functions with nested variable dependency. Currently, users have to define the function with no intermediate lines. One future improvement is to enable support for nested dependency so that there is a fully functional Reverse Mode that supports any functions.\n",
    "\n",
    "Secondly, a full-fledged backpropagation support could be provided as an additional feature. Backpropagation used in deep learning generally involves matrix or tensor. However, the current `AutoDiff` implementation does not support those kinds of input. In the future we will extend our package to support those data inputs and provide a convenient wrapper for commonly-used operations such as linear map, convolution layers and attention module, as well as activation function and several loss functions.\n",
    "\n",
    "Additionally, two optimizers are provided as additional features, namely Newton method and SGD. However there are many more interesting optimizers such as Adam and AdaFactor. Additional optimizers can also be implemented for the `AutoDiff` package as a future step.\n",
    "\n",
    "Moreover, the current computation graph visualization is static, meaning that the generated computation graph is a fixed image. This visualization can be made more interactive, for example, users could drag the nodes or modify the graph on the fly.\n",
    "\n",
    "Lastly, since the `AutoDiff` package supports computing gradient and Jacobian, there are many interesting gradient based samplings to explore. For example, Langvian dynamics samples from gradients of evidence. Sampled Langvian dynamics is a path towards the mode of two Gaussian mixtures. Providing those sampling to users could also be a nice functionality to have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NA1D18XrYye9"
   },
   "source": [
    "## **9.0 License**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Fn5KXPXCcg2"
   },
   "source": [
    "We chose to use the MIT License. As described by the MIT license, it will allow users to reuse the code for commercial or private use, distribution and modification (essentially any purpose), as long as the users include the original copy of the MIT license in their distribution. The MIT license is a permissive license as it does not require the user to make their work publicly available as well. Automatic differentiation is a project that has been worked on by many users, i.e. the software is not very substantial, and the project can be very useful for anyone wants to differentiate their functions quickly. Since this is a course project, we do not want to profit from the software, but we want to use the software as a component of a broader service. Therefore we do not feel the need to limit the use of our software, and it is not necessary for us to use a copyleft license to force out user to make their project open source. Due to similar reasons, we do not care about patents."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf1ee66cb0a5dbbe9b15b35d8b5ef163dfe6de55254de5ac3d70cf481ad0a057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
